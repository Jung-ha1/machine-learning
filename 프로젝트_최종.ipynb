{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "프로젝트 최종",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFRyXf9tdrYGEwW9BkqK81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jung-ha1/machine-learning/blob/main/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_%EC%B5%9C%EC%A2%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ybBXdbvQHr"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "arr = ['h', 'i', 'e', 'l', 'o']\n",
        "\n",
        "y_data = [[1, 0, 2, 3, 3, 4]]\n",
        "\n",
        "num_classes = 5\n",
        "input_dim = 5 \n",
        "sequence_length = 6\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "x_one_hot = np.array([[[1, 0, 0, 0, 0],    # h 0\n",
        "                       [0, 1, 0, 0, 0],    # i 1\n",
        "                       [1, 0, 0, 0, 0],    # h 0\n",
        "                       [0, 0, 1, 0, 0],    # e 2\n",
        "                       [0, 0, 0, 1, 0],    # l 3\n",
        "                       [0, 0, 0, 1, 0]]],  # l 3\n",
        "                     dtype=np.float32)\n",
        "\n",
        " \n",
        "y_one_hot = tf.keras.utils.to_categorical(y_data, num_classes=num_classes)\n",
        "\n",
        "print(x_one_hot.shape)\n",
        "print(y_one_hot.shape)\n",
        "\n",
        " \n",
        "\n",
        "tf.model = tf.keras.Sequential()\n",
        "\n",
        "cell = tf.keras.layers.LSTMCell(units=num_classes, input_shape=(sequence_length, input_dim))\n",
        "\n",
        "tf.model.add(tf.keras.layers.RNN(cell=cell, return_sequences=True))\n",
        "\n",
        "tf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes, activation='softmax'))).\n",
        "\n",
        "tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "tf.model.fit(x_one_hot, y_one_hot, epochs=50)\n",
        "tf.model.summary()\n",
        "\n",
        "predictions = tf.model.predict(x_one_hot)\n",
        "\n",
        "\n",
        "for i, prediction in enumerate(predictions):\n",
        "    print(prediction)\n",
        "    result_str = [arr[c] for c in np.argmax(prediction, axis=1)]\n",
        "    print(\"\\tPrediction str: \", ''.join(result_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mUao_RKxEfU"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "arr = ['h', 'i', 'e', 'l', 'o']\n",
        "# 각각의 알파벳에 one-hot 인코딩을 하기 위해 배열을 만들어 준다.\n",
        "\n",
        "y_data = [[1, 0, 2, 3, 3, 4]]\n",
        "# y_data는 arr배열의 인덱스이며, y_data의 값을 부르게 되면 ihello라는 결과가 나옵니다.\n",
        "# 즉 x_data를 선언할 때 예시로 hileol을 학습시키면 ihello와 같이 나오도록 학습데이터를 선언해 준 것 입니다.\n",
        " \n",
        "\n",
        "num_classes = 5   # hielo를 분류한다.\n",
        "input_dim = 5 \n",
        "sequence_length = 6   # y_data의 길이\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "x_one_hot = np.array([[[1, 0, 0, 0, 0],    # h 0\n",
        "                       [0, 1, 0, 0, 0],    # i 1\n",
        "                       [1, 0, 0, 0, 0],    # h 0\n",
        "                       [0, 0, 1, 0, 0],    # e 2\n",
        "                       [0, 0, 0, 1, 0],    # l 3\n",
        "                       [0, 0, 0, 1, 0]]],  # l 3\n",
        "                     dtype=np.float32)\n",
        "# x_data를 학습시킬 때 x_data를 원핫 인코딩한 값으로 정해줍니다.\n",
        "\n",
        " \n",
        "y_one_hot = tf.keras.utils.to_categorical(y_data, num_classes=num_classes)\n",
        "# y_data를 원핫 인코딩을 해줘야하는데, 즉 i= [0 0 0 0 0], h = [0 0 0 0 1]...과 같이 지정을 해줍니다.\n",
        "\n",
        "print(x_one_hot.shape)\n",
        "print(y_one_hot.shape)\n",
        "\n",
        " \n",
        "\n",
        "tf.model = tf.keras.Sequential()\n",
        "\n",
        "cell = tf.keras.layers.LSTMCell(units=num_classes, input_shape=(sequence_length, input_dim))\n",
        "# RNN 중 한개인 LSTM으로 input data를 6행 5열로 정렬해줍니다.\n",
        "\n",
        "tf.model.add(tf.keras.layers.RNN(cell=cell, return_sequences=True))\n",
        "# LSTM의 cell을 지정하고 Sequences = True란 명령어로 Time sequential한 데이터임을 선언해줍니다.\n",
        "\n",
        "tf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes, activation='softmax')))\n",
        "# 수업에서 배웠던 ReLU 함수는 0이상의 값에서 y=x란 식이 나오며, 대부분 발산을 합니다.\n",
        "# 그런데 RNN은 과거의 데이터를 지속적으로 꺼내 사용하기 때문에 -1에서 1사이로 normalizing이 필요합니다. \n",
        "# 따라서 output layer의 값을 -1에서 1사이로 normalizing해줄 수 있는 softmax을 사용해 줍니다.\n",
        "\n",
        "tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "tf.model.fit(x_one_hot, y_one_hot, epochs=50)\n",
        "tf.model.summary()\n",
        "\n",
        "predictions = tf.model.predict(x_one_hot)\n",
        "\n",
        "\n",
        "for i, prediction in enumerate(predictions):\n",
        "    print(prediction)\n",
        "    result_str = [arr[c] for c in np.argmax(prediction, axis=1)]\n",
        "    print(\"\\tPrediction str: \", ''.join(result_str))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}